{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "# Técnicas Matemáticas para Big Data - Project 04\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "JiO-DQt1alth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GROUP 06:\n",
        "- Joana Daniela Carvalho Rodrigues - Nº 112926 - xx% Work Participation\n",
        "- Liliana Paula Cruz Ribeiro - Nº 108713 - xx% Work Participation\n",
        "- Willian Pegorin - Nº 122970 - xx% Work Participation"
      ],
      "metadata": {
        "id": "JQ4ykoJ3axBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "## **Q1:**  What is the numeric criteria that you may use to determine if a change in the algorithm produces improvements?"
      ],
      "metadata": {
        "id": "J-uTVMdk_cg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine if a change to the SOM algorithm leads to improvements, we must use quantitative performance measures. A commonly used metric is the **Quantization Error (QE)**, which measures the average distance between each input vector $x_i$ and its corresponding **Best Matching Unit (BMU)** after training. This metric evaluates how accurately the SOM represents the unput data distribution. The QE is given by the following expression:\n",
        "$$\n",
        "QE = \\dfrac{1}{N}\\sum_{i=1}^{N}\\|x_i-w_{BMU(i)}\\|\n",
        "$$\n",
        "where $N$ is the number of input samples and $w_{BMU(i)}$ is the weight vector of the BMU associated with $x_i$.\n",
        "\n",
        "However, QE alone does not evaluate the preservation of the input space topology. For this reason, the Topographic Error (TE) is also commonly used. It measures the proportion of data samples for which the first and second BMUs are not adjacent in the map also quantifying topological distortions.\n",
        "\n",
        "Formally, the TE is defined as:\n",
        "$$\n",
        "TE = \\dfrac{1}{N} \\sum_{i=1}^N u(x_i),\n",
        "$$\n",
        "where $u(x_i) = 1$ if the first and second BMUs of $x_i$ are not adjacent and $u(x_i) = 0$ otherwise. The TE takes values in the interval $[0,1]$, with lower values indicating better topological preservation.\n",
        "\n",
        "An algorithmic change may be considered an improvement if it results in a lower QE while maintaining or reducing the TE."
      ],
      "metadata": {
        "id": "YAi0z4DA71JF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "## **Q2:** Write the version SOM1A, where you change the curve of the learning factor. Did you achieve improvements?"
      ],
      "metadata": {
        "id": "hEo0H3NU_9Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In version SOM1A, we changed the curve of the learning factor to a linear decay instead of exponencial in file \"our_som1A\", defined as:\n",
        "\n",
        "$$\n",
        "\\alpha(t) = \\alpha_0\\left(1-\\dfrac{t}{T}\\right)\n",
        "$$\n",
        "where $\\alpha_0$ is the initial learning rate and $T$ is the total number of training iterations."
      ],
      "metadata": {
        "id": "eqxbY5KoUFyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decay_learning_rate(self, initial_learning_rate, iteration, num_iterations):\n",
        "        return initial_learning_rate * (1 - iteration / num_iterations)"
      ],
      "metadata": {
        "id": "WoCdTrtwUvXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the impact of this modification, we used two SOMs with the same initialization seed and number of epochs, but one was trained using the original exponencial decay and the other using the linear decay. To understand if there was an improvement, we used QE and TE."
      ],
      "metadata": {
        "id": "QTfogbs4B0Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches as patches\n",
        "import matplotlib.lines as mlines\n",
        "\n",
        "# reading data\n",
        "data = pd.read_csv(\"cash-crops-nepal.csv\")\n",
        "agri_data = data.iloc[np.random.permutation(len(data))]\n",
        "trunc_data = agri_data[[\"Area\", \"Production\", \"Yield\"]]\n",
        "trunc_data = trunc_data / trunc_data.max()\n",
        "\n",
        "from our_som1A import SOM_A\n",
        "from our_som1 import SOM\n",
        "\n",
        "# som = SOM(x_size, y_size, num_features)\n",
        "np.random.seed(42)\n",
        "agri_som_lin = SOM_A(6,6,3)\n",
        "\n",
        "# Initial weights\n",
        "init_fig = plt.figure()\n",
        "agri_som_lin.show_plot(init_fig, 1, 0)\n",
        "plt.show()\n",
        "\n",
        "agri_som_lin.train(trunc_data.values,\n",
        "          num_epochs=200,\n",
        "          init_learning_rate=0.01\n",
        "          )\n",
        "\n",
        "# Função para calcular métricas\n",
        "def evaluate_som(som, data):\n",
        "    \"\"\"\n",
        "    Calcula métricas de qualidade do SOM\n",
        "    \"\"\"\n",
        "    # 1. Erro de Quantização (Quantization Error)\n",
        "    quantization_error = 0\n",
        "    for idx, row in data.iterrows():\n",
        "        bmu, _ = som.find_bmu(row.values)\n",
        "        quantization_error += np.sum((row.values - bmu) ** 2)\n",
        "    quantization_error /= len(data)\n",
        "\n",
        "    # 2. Erro Topográfico (Topographic Error)\n",
        "    topographic_error = 0\n",
        "    for idx, row in data.iterrows():\n",
        "        # Encontrar os dois BMUs mais próximos\n",
        "        distances = []\n",
        "        for x in range(som.network_dimensions[0]):\n",
        "            for y in range(som.network_dimensions[1]):\n",
        "                w = som.net[x, y, :]\n",
        "                dist = np.sum((row.values - w) ** 2)\n",
        "                distances.append(((x, y), dist))\n",
        "\n",
        "        # Ordenar por distância\n",
        "        distances.sort(key=lambda x: x[1])\n",
        "        bmu1, bmu2 = distances[0][0], distances[1][0]\n",
        "\n",
        "        # Verificar se são adjacentes (Manhattan distance <= 1)\n",
        "        if abs(bmu1[0] - bmu2[0]) + abs(bmu1[1] - bmu2[1]) > 1:\n",
        "            topographic_error += 1\n",
        "\n",
        "    topographic_error /= len(data)\n",
        "\n",
        "\n",
        "    return {\n",
        "        'quantization_error': quantization_error,\n",
        "        'topographic_error': topographic_error\n",
        "    }\n",
        "\n",
        "\n",
        "metrics_lin = evaluate_som(agri_som_lin, trunc_data)\n",
        "\n",
        "np.random.seed(42)\n",
        "agri_som_exp = SOM(6, 6, 3)\n",
        "agri_som_exp.train(trunc_data.values, num_epochs=200, init_learning_rate=0.01)\n",
        "metrics_exp = evaluate_som(agri_som_exp, trunc_data)\n",
        "\n",
        "\n",
        "print(f\"\\n{'Métrica':<30} {'Linear':<15} {'Exponencial':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(f\"{'QE':<30} {metrics_lin['quantization_error']:<15.6f} \"\n",
        "      f\"{metrics_exp['quantization_error']:<15.6f}\")\n",
        "\n",
        "print(f\"{'Topographic error':<30} {metrics_lin['topographic_error']:<15.6f} \"\n",
        "      f\"{metrics_exp['topographic_error']:<15.6f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R3Bj5ndsWPT-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d966ea02-5b10-4e9e-fad3-866bba4ba94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'cash-crops-nepal.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-245652703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# reading data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cash-crops-nepal.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0magri_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrunc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magri_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Area\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Production\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Yield\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cash-crops-nepal.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These were the results:"
      ],
      "metadata": {
        "id": "g51WA_PjC8Nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric | Linear | Exponencial |\n",
        "| ------ | :----: | :---------: |\n",
        "| QE | 0.004088 | 0.002417 |\n",
        "| TE | 0.142857 | 0.152381 |"
      ],
      "metadata": {
        "id": "v4Yd7ALxXExm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing the table above, the results indicate that the linear decay leads to a slight improvement in topographic preservation due to a lower TE. However, it has a higher QE.\n",
        "\n",
        "Therefore, the modification doesn't result in an overall improvement. It seems that the linear decay reduces the learning rate to aggressively, resulting in a reduced ability to perform subtle weight adjustments toward the end of the training process. On the other hand, the exponential decay provides a better balance between global ordering and local convergence."
      ],
      "metadata": {
        "id": "GLydB0XtXuvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "## **Q3:** Write the version SOM1B, where you change the curve of the deviation. Did you achieve improvements?"
      ],
      "metadata": {
        "id": "eTJLB7rL_9ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Self-Organizing Map algorithm, the neighborhood deviation σ(t) controls the spatial extent of the neighborhood function and regulates the balance between global ordering and local adaptation. Larger values of σ(t) promote smoother global updates, while smaller values emphasize local refinement around the Best Matching Unit (BMU). Therefore, the decay profile of σ(t) directly influences the convergence dynamics of the algorithm.\n",
        "\n",
        "In the provided implementation, the deviation parameter σ(t) is implicitly represented by the neighborhood radius variable. The SOM1B version was implemented by modifying exclusively the decay function of this radius, replacing the original exponential decay with a linear decay combined with a lower bound, while keeping the Gaussian neighborhood function unchanged.\n"
      ],
      "metadata": {
        "id": "m3Suvl7eFLLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code snippet illustrates the modification introduced in the SOM1B version:\n",
        "```python\n",
        "<our_som1B.py>\n",
        "class SOM_B:\n",
        "...\n",
        "    def decay_radius(self, iteration):\n",
        "        #return self.init_radius * np.exp(-iteration / self.time_constant)\n",
        "        # Decaimento linear\n",
        "        radius = self.init_radius * (1 - iteration / self.time_constant)\n",
        "        return max(radius, 1.0)"
      ],
      "metadata": {
        "id": "vLIY0rUIFSyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### version SOM1B\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches as patches\n",
        "import matplotlib.lines as mlines\n",
        "\n",
        "# reading data\n",
        "data = pd.read_csv(\"cash-crops-nepal.csv\")\n",
        "agri_data = data.iloc[np.random.permutation(len(data))]\n",
        "trunc_data = agri_data[[\"Area\", \"Production\", \"Yield\"]]\n",
        "trunc_data = trunc_data / trunc_data.max()\n",
        "\n",
        "from our_som1B import SOM_B\n",
        "from our_som1 import SOM\n",
        "\n",
        "# som = SOM(x_size, y_size, num_features)\n",
        "np.random.seed(42)\n",
        "agri_som_decay_radius_lin = SOM_B(6,6,3)\n",
        "\n",
        "# Initial weights\n",
        "init_fig = plt.figure()\n",
        "agri_som_decay_radius_lin.show_plot(init_fig, 1, 0)\n",
        "plt.show()\n",
        "\n",
        "agri_som_decay_radius_lin.train(trunc_data.values,\n",
        "          num_epochs=200,\n",
        "          init_learning_rate=0.01\n",
        "          )\n",
        "\n",
        "# Function to compute SOM quality metrics\n",
        "def evaluate_som(som, data):\n",
        "    \"\"\"\n",
        "    Computes quality metrics for a trained Self-Organizing Map (SOM).\n",
        "\n",
        "    Metrics:\n",
        "    1. Quantization Error (QE)\n",
        "    2. Topographic Error (TE)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Quantization Error (QE)\n",
        "    quantization_error = 0.0\n",
        "    for _, row in data.iterrows():\n",
        "        bmu, _ = som.find_bmu(row.values)\n",
        "        quantization_error += np.sum((row.values - bmu) ** 2)\n",
        "    quantization_error /= len(data)\n",
        "\n",
        "    # 2. Topographic Error (TE)\n",
        "    topographic_error = 0.0\n",
        "    for _, row in data.iterrows():\n",
        "        # Find the two closest BMUs\n",
        "        distances = []\n",
        "        for x in range(som.network_dimensions[0]):\n",
        "            for y in range(som.network_dimensions[1]):\n",
        "                w = som.net[x, y, :]\n",
        "                dist = np.sum((row.values - w) ** 2)\n",
        "                distances.append(((x, y), dist))\n",
        "\n",
        "        # Sort by distance\n",
        "        distances.sort(key=lambda x: x[1])\n",
        "        bmu1, bmu2 = distances[0][0], distances[1][0]\n",
        "\n",
        "        # Check if the two BMUs are adjacent (Manhattan distance <= 1)\n",
        "        if abs(bmu1[0] - bmu2[0]) + abs(bmu1[1] - bmu2[1]) > 1:\n",
        "            topographic_error += 1\n",
        "\n",
        "    topographic_error /= len(data)\n",
        "\n",
        "    return {\n",
        "        \"quantization_error\": quantization_error,\n",
        "        \"topographic_error\": topographic_error\n",
        "    }\n",
        "\n",
        "metrics_radius_lin = evaluate_som(agri_som_decay_radius_lin, trunc_data)\n",
        "\n",
        "np.random.seed(42)\n",
        "agri_som_decay_radius_exp = SOM(6, 6, 3)\n",
        "agri_som_decay_radius_exp.train(trunc_data.values, num_epochs=200, init_learning_rate=0.01)\n",
        "metrics_radius_exp = evaluate_som(agri_som_decay_radius_exp, trunc_data)\n",
        "\n",
        "\n",
        "print(f\"\\n{'Metric':<30} {'Radius Exponential':<15} {'Radius Linear':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(f\"{'QE':<30} {metrics_radius_exp['quantization_error']:<15.6f} \"\n",
        "      f\"{metrics_radius_lin['quantization_error']:<15.6f}\")\n",
        "\n",
        "print(f\"{'Topographic error':<30} {metrics_radius_exp['topographic_error']:<15.6f} \"\n",
        "      f\"{metrics_radius_lin['topographic_error']:<15.6f}\")"
      ],
      "metadata": {
        "id": "SnGR0YtgFX13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Epoch | SOM1 (Radius Exponential Decay) | SOM1B (Radius Linear Decay + Cutoff) |\n",
        "| ----- | :------------------: | :----------------------: |\n",
        "| 20    | 5.02               | 4.92                   |\n",
        "| 40    | 4.19               | 3.85                   |\n",
        "| 60    | 3.51               | 2.77                   |\n",
        "| 80    | 2.93               | 1.70                   |\n",
        "| 100   | 2.45               | **1.0**                |\n",
        "| 120   | 2.05               | **1.0**                |\n",
        "| 140   | 1.71               | **1.0**                |\n",
        "| 160   | 1.43               | **1.0**                |\n",
        "| 180   | 1.20               | **1.0**                |\n",
        "| 200   | **1.0**            | **1.0**                |\n",
        "\n",
        "**Table Q3.1:** Comparison of the evolution of the neighborhood radius (σ(t)) between the original SOM and the SOM1B version.\n",
        "\n",
        "\n",
        "| Metric | Radius Exponential Decay | Radius Linear Decay |\n",
        "| ------ | :----------------: | :-----------: |\n",
        "| QE | 0.002426 | 0.002013 |\n",
        "| TE | 0.161905 | 0.142857 |\n",
        "\n",
        "**Table Q3.2:** Comparison of the evolution of numeric criteria (Quantitization Erro **QE** and Topological Error **TE**) between the original SOM and the SOM1B version."
      ],
      "metadata": {
        "id": "i7DBlnM1Fayx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in Table Q3.1, the linear decay introduced in SOM1B leads to a faster reduction of the neighborhood radius, reaching its minimum value around epoch 100, whereas the original SOM requires approximately 200 epochs. This behavior accelerates the transition from global ordering to local fine-tuning, promoting earlier local adaptation of the map.\n",
        "\n",
        "According to the numerical results reported in Table Q3.2, the final quantization error obtained with the modified deviation curve is lower than that of the original SOM, indicating an improved representation of the input data. The topographic error also presents a slight reduction, suggesting a modest improvement in topological consistency.\n",
        "\n",
        "From a training dynamics perspective, this modification can be regarded as beneficial, since the earlier reduction of the neighborhood radius limits the influence of distant neurons and reduces excessive smoothing in later epochs. As a consequence, the map converges more rapidly toward a locally consistent organization.\n",
        "\n",
        "On the other hand, the faster decay of the neighborhood radius shortens the phase dedicated to global organization, which may negatively affect large-scale topological preservation in certain datasets. Therefore, the observed improvement is mainly associated with convergence speed and locality, rather than a guaranteed enhancement of global mapping quality.\n",
        "\n",
        "Overall, under the adopted numerical criterion, the SOM1B version can be considered an improvement over the original SOM."
      ],
      "metadata": {
        "id": "x_Rd3HxLFjqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "## **Q4:** Write the version SOM1C, where you change the change the normal distribution to other distribution of your\n",
        "choice. Did you achieve improvements?"
      ],
      "metadata": {
        "id": "2zD_VeQQ_9_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Self-Organizing Map algorithm, the neighborhood function defines how the update of the winning neuron propagates to its neighbors. In the original formulation, this influence is typically modeled using a Gaussian distribution, which provides a smooth and rapidly decaying spatial interaction.\n",
        "\n",
        "In the SOM1C version, the Gaussian neighborhood function was replaced by an exponential distribution, while keeping the learning rate and neighborhood radius decay unchanged. This modification directly affects how the influence of the Best Matching Unit (BMU) spreads across the map during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "1hWbLfz0JGgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code snippet illustrates the modification introduced in the SOM1C version, where the Gaussian neighborhood function was replaced by an exponential distribution.\n",
        "```python\n",
        "<our_som1C.py>\n",
        "class SOM_C:\n",
        "...\n",
        "    @staticmethod\n",
        "    def calculate_influence(distance, radius):\n",
        "        #return np.exp(-distance / (2 * (radius ** 2)))\n",
        "        # exponential distribution\n",
        "        return np.exp(-np.sqrt(distance) / radius)"
      ],
      "metadata": {
        "id": "scehLbVQJOSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### version SOM1C\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches as patches\n",
        "import matplotlib.lines as mlines\n",
        "\n",
        "# reading data\n",
        "data = pd.read_csv(\"cash-crops-nepal.csv\")\n",
        "agri_data = data.iloc[np.random.permutation(len(data))]\n",
        "trunc_data = agri_data[[\"Area\", \"Production\", \"Yield\"]]\n",
        "trunc_data = trunc_data / trunc_data.max()\n",
        "\n",
        "from our_som1C import SOM_C\n",
        "from our_som1 import SOM\n",
        "\n",
        "# som = SOM(x_size, y_size, num_features)\n",
        "np.random.seed(42)\n",
        "agri_som_dist_exponential = SOM_C(6,6,3)\n",
        "\n",
        "# Initial weights\n",
        "init_fig = plt.figure()\n",
        "agri_som_dist_exponential.show_plot(init_fig, 1, 0)\n",
        "plt.show()\n",
        "\n",
        "agri_som_dist_exponential.train(trunc_data.values,\n",
        "          num_epochs=200,\n",
        "          init_learning_rate=0.01\n",
        "          )\n",
        "\n",
        "#evaluate_som is defined in Questions 1/2 and reused here\n",
        "\n",
        "metrics_dist_exponential = evaluate_som(agri_som_dist_exponential, trunc_data)\n",
        "\n",
        "np.random.seed(42)\n",
        "agri_som_dist_gaussian = SOM(6, 6, 3)\n",
        "agri_som_dist_gaussian.train(trunc_data.values, num_epochs=200, init_learning_rate=0.01)\n",
        "metrics_dist_gaussian = evaluate_som(agri_som_dist_gaussian, trunc_data)\n",
        "\n",
        "\n",
        "print(f\"\\n{'Metric':<30} {'Dist Gaussian':<15} {'Dist Exponential':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(f\"{'QE':<30} {metrics_dist_gaussian['quantization_error']:<15.6f} \"\n",
        "      f\"{metrics_dist_exponential['quantization_error']:<15.6f}\")\n",
        "\n",
        "print(f\"{'Topographic error':<30} {metrics_dist_gaussian['topographic_error']:<15.6f} \"\n",
        "      f\"{metrics_dist_exponential['topographic_error']:<15.6f}\")"
      ],
      "metadata": {
        "id": "OHxvQVLIJPoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Metric | Dist Gaussian | Dist Exponential |\n",
        "| ------ | :----------------: | :-----------: |\n",
        "| QE | 0.002417 | 0.002124 |\n",
        "| TE | 0.152381 | 0.152381 |\n",
        "\n",
        "**Table Q4.1:** Comparison of the evolution of numeric criteria (Quantitization Erro **QE** and Topological Error **TE**) between the original SOM and the SOM1C version."
      ],
      "metadata": {
        "id": "zKVISDEkJTkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table Q4.1 reports the quantitative comparison between the Gaussian and exponential neighborhood functions using the numerical criteria defined in Question 1. The results show that the exponential neighborhood function yields a slightly lower quantization error, indicating a modest improvement in how the SOM prototypes represent the input data.\n",
        "\n",
        "Regarding topological preservation, the topographic error remains unchanged for both neighborhood functions. This suggests that, under the considered training setup, replacing the Gaussian kernel with an exponential one does not significantly affect the global topological organization of the map.\n",
        "\n",
        "From an algorithmic perspective, the exponential neighborhood function decays more slowly with distance from the BMU, allowing neurons farther away to retain influence during the update process, particularly in the early stages of training. While this can lead to smoother spatial adaptations, its impact on topological consistency appears to be limited for the present dataset.\n",
        "\n",
        "Overall, the SOM1C modification can be considered a mild improvement in terms of quantization performance, while maintaining a similar level of topological consistency when compared to the original Gaussian-based SOM.\n"
      ],
      "metadata": {
        "id": "dYJaJlDIJZOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "## **Q5$^*$:** Determine the mathematical conditions that ensure the convergence of equation (3) in page 14 of this slides."
      ],
      "metadata": {
        "id": "e866a6a8_-Yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the following equation\n",
        "$$\n",
        "w_k(t+1) = w_k(t)+\\alpha(t)h_{ck}(t)[x(t)-w_k(t)]\n",
        "$$\n",
        "where $\\alpha(t)$ is the learning-rate factor and $h_{ck}(t)$ is the neighborhood function, defined as:\n",
        "$$\n",
        "h_{ck}(t)= exp\\left(\\dfrac{-\\|r_k-r_c\\|^2}{σ(t)^2}\\right)\n",
        "$$\n",
        "\n",
        "where $r_k$ and $r_c$ denote the coordinates of nodes $k$ and $c$, respectively on the two-dimensional lattice.\n",
        "\n",
        "The first equation is a stochastic aproximation process and its convergence can be analyzed using classical results from stochastic approximation theory combined with properties specific to the SOM algorithm.\n",
        "\n",
        "That way, the convergence of this process can be ensured under the following conditions:\n",
        "1. **Learning-rate conditions (Robbins-Monro):**\n",
        "$$\n",
        "\\sum_{t=1}^∞α(t)=∞,\\space \\sum_{t=1}^∞α^2(t)<∞\n",
        "$$\n",
        "These conditions guarantee sufficient exploration while preventing divergence.\n",
        "\n",
        "2. **Neighborhood decay:**\n",
        "$$\n",
        "\\sigma(t) → 0 \\space \\text{ as } \\space t → ∞,\n",
        "$$\n",
        "which implies that\n",
        "$$\n",
        "h_{ck}(t) ⟶ δ_{ck},\n",
        "$$\n",
        "where $δ_{ck}$ corresponds to the Kronecker delta. This ensures that only the winning neuron is updated.\n",
        "\n",
        "3. **Boundedness assumptions**\n",
        "   - the input vectors $x(t)$ are drawn from a stationary distribution;\n",
        "   - $\\|x(t)\\| \\leq C < ∞$;\n",
        "   - the weight vectors remain bounded: $\\sup_{t} \\|w_k(t)\\| < \\infty$.\n",
        "\n",
        "Under these conditions, the first equation converges almost surely to a stable configuration corresponding to a local minimum of the distortion measure. As the neighborhood radius collapses, the algorithm asymptotically behaves like stochastic $k$-means clustering. While convergence of the weights is guaranteed, convergence to the global optimum is not."
      ],
      "metadata": {
        "id": "aRlReKe-HES1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "## **Q6:** As explained in class, SOM can be seen as a Euler integration method for the corresponding ODE. Estimate the absolute error after N epochs.\n",
        "\n"
      ],
      "metadata": {
        "id": "9fDo5zkv_-vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Self-Organizing Map (SOM) update rule can be interpreted as a numerical integration scheme for an underlying ordinary differential equation (ODE). Specifically, the standard SOM weight update,\n",
        "$$\n",
        "w_k(t+1) = w_k(t) + \\alpha(t)\\,h_{ck}(t)\\,[x(t) - w_k(t)],\n",
        "$$\n",
        "corresponds to the explicit Euler method applied to the continuous-time ODE\n",
        "$$\n",
        "\\frac{dw_k}{dt} = h_{ck}(t)\\,[x(t) - w_k(t)] = f(t, w_k).\n",
        "$$\n",
        "\n",
        "In this formulation, the learning rate $\\alpha(t)$ plays the role of the step size $h$ in the Euler discretization.\n",
        "\n",
        "The Euler method is derived from the Taylor expansion, where the local truncation error corresponds to the neglected term $\\frac{h^2}{2}y''(\\xi)$. Therefore, the local truncation error at each step is:\n",
        "$$\n",
        "e_{\\text{local}}(t) = \\frac{\\alpha(t)^2}{2} \\left|\\frac{d^2 w_k}{dt^2}\\right|\n",
        "$$\n",
        "\n",
        "The second derivative can be computed as:\n",
        "$$\n",
        "\\frac{d^2 w_k}{dt^2} = -h_{ck}(t)^2(x(t) - w_k(t))\n",
        "$$\n",
        "\n",
        "The absolute error after N epochs is estimated by accumulating the local errors:\n",
        "$$\n",
        "E_N = \\sum_{t=1}^{T} e_{\\text{local}}(t) = \\sum_{t=1}^{T} \\frac{\\alpha(t)^2}{2} \\left|h_{ck}(t)^2(x(t) - w_k(t))\\right|\n",
        "$$\n",
        "where $T = N \\cdot M$ is the total number of updates ($N$ epochs, $M$ samples per epoch)."
      ],
      "metadata": {
        "id": "QiGvCPLvks6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from our_som1 import SOM\n",
        "\n",
        "np.random.seed(42)\n",
        "som = SOM(6, 6, 3)\n",
        "\n",
        "total_error = 0.0\n",
        "num_epochs = 200\n",
        "init_learning_rate = 0.01\n",
        "som.time_constant = num_epochs / np.log(som.init_radius)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    radius = som.decay_radius(epoch)\n",
        "    alpha = som.decay_learning_rate(init_learning_rate, epoch, num_epochs)\n",
        "\n",
        "    for idx, row in trunc_data.iterrows():\n",
        "        x_t = row.values\n",
        "        bmu, bmu_idx = som.find_bmu(x_t)\n",
        "\n",
        "        for i in range(6):\n",
        "            for j in range(6):\n",
        "                w_k = som.net[i, j, :]\n",
        "                w_dist = np.sum((np.array([i, j]) - bmu_idx) ** 2)\n",
        "\n",
        "                if w_dist <= radius ** 2:\n",
        "                    h_ck = np.exp(-w_dist / (2 * radius ** 2))\n",
        "\n",
        "                    # error local\n",
        "                    local_error = (alpha**2 / 2) * np.sum(np.abs(h_ck**2 * (x_t - w_k)))\n",
        "                    total_error += local_error\n",
        "\n",
        "                    # weight update\n",
        "                    som.net[i, j, :] = w_k + alpha * h_ck * (x_t - w_k)\n",
        "\n",
        "print(f\"Estimated absolute error (Euler): {total_error:.6f}\")"
      ],
      "metadata": {
        "id": "2LWMgLMEldID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "## **Q7$^*$:**  How could you change the SOM method to use Runge-Kutta second order method? Is the improvements?"
      ],
      "metadata": {
        "id": "vfYJHiCs__GI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classical Self-Organizing Map (SOM) algorithm can be understood through the lens of numerical analysis. Fundamentally, its weight update mechanism implements a numerical integration scheme for an underlying ordinary differential equation (ODE) that governs the evolution of the prototype vectors. In its standard formulation, this scheme corresponds to the **explicit Euler method**, which is a first-order numerical integrator.\n",
        "\n",
        "The conventional SOM update rule,\n",
        "$$\n",
        "w_k(t+1) = w_k(t) + \\alpha(t)\\,h_{ck}(t)\\,[x(t) - w_k(t)],\n",
        "$$\n",
        "can be interpreted as a direct time discretization of the continuous-time ODE\n",
        "$$\n",
        "\\frac{dw_k}{dt} = h_{ck}(t)\\,[x(t) - w_k(t)].\n",
        "$$\n",
        "\n",
        "Since the explicit Euler method is well known for its limited accuracy and conditional stability, a natural way to improve the numerical behavior of the algorithm is to adopt a higher-order integration scheme. In the context, replacing the Euler update with a **second-order Runge–Kutta method (RK2)** provides a meaningful enchancement. This two-stage approach, consisting of a  predictor step followed by a corrector step that averages the estimated slopes, yields a more faithful approximation of the underlying continuous dynamics.\n",
        "\n",
        "When applied to the SOM setting, the RK2 update can be written as:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "k_1 &= h_{ck}(t)\\,[x(t) - w_k(t)], \\\\[4pt]\n",
        "w_k^{*} &= w_k(t) + \\alpha(t)\\,k_1, \\\\[4pt]\n",
        "k_2 &= h_{ck}(t)\\,[x(t) - w_k^{*}], \\\\[4pt]\n",
        "w_k(t+1) &= w_k(t) + \\frac{\\alpha(t)}{2}\\,(k_1 + k_2).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "From a numerical analysis perspective, the main advantage of this modification lies in the order of the error. While the explicit Euler method exhibits a global truncation error of order $\\mathcal{O}(h)$, the RK2 scheme achieves a global error of order $\\mathcal{O}(h^2)$. As a result, for a given effective step size (or learning rate), the RK2-based SOM provides a more accurate integration of the governing ODE. In practice, this leads to smoother and more stable trajectories for the weight vectors during the learning process.\n",
        "\n",
        "Although the RK2 variant introduces a modest increase in computational cost per iteration due to the additional function evaluation, this overhead is typically justified. The improved numerical stability and the potential gains in convergence robustness make the second-order Runge–Kutta approach a principled and effective refinement of the classical SOM algorithm."
      ],
      "metadata": {
        "id": "r0xyzCQZbbSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code snippet illustrates the modification introduced in the version, where the conventional SOM update rule was replaced by the RK2 update.\n",
        "```python\n",
        "<our_som1_RK2.py>\n",
        "class SOM_RK2:\n",
        "...\n",
        "    def train(self, data, num_epochs=100, init_learning_rate=0.01, resetWeights=False):\n",
        "        ...\n",
        "            for record in indices:\n",
        "                ...\n",
        "                for x in range(self.network_dimensions[0]):\n",
        "                    for y in range(self.network_dimensions[1]):\n",
        "                       ...\n",
        "                        if w_dist <= radius ** 2:\n",
        "                            # update weight vectors wk using Eq. (3)\n",
        "                            influence = SOM_RK2.calculate_influence(w_dist, radius)\n",
        "                            #second-order Runge–Kutta method (RK2)\n",
        "                            new_w = SOM_RK2.rk2_update(weight, row_t, learning_rate, influence)\n",
        "                            self.net[x, y, :] = new_w.reshape(1, self.num_features)\n",
        "\n",
        "    @staticmethod\n",
        "    def rk2_update(weight, x_t, alpha, influence):\n",
        "        k1 = influence * (x_t - weight)\n",
        "        w_star = weight + alpha * k1\n",
        "        k2 = influence * (x_t - w_star)\n",
        "        return weight + (alpha / 2.0) * (k1 + k2)"
      ],
      "metadata": {
        "id": "r8L9X3VvbsqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### version SOM1_RK2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches as patches\n",
        "import matplotlib.lines as mlines\n",
        "\n",
        "# reading data\n",
        "data = pd.read_csv(\"cash-crops-nepal.csv\")\n",
        "agri_data = data.iloc[np.random.permutation(len(data))]\n",
        "trunc_data = agri_data[[\"Area\", \"Production\", \"Yield\"]]\n",
        "trunc_data = trunc_data / trunc_data.max()\n",
        "\n",
        "from our_som1_RK2 import SOM_RK2\n",
        "from our_som1 import SOM\n",
        "\n",
        "# som = SOM(x_size, y_size, num_features)\n",
        "np.random.seed(42)\n",
        "agri_som_RK2 = SOM(6,6,3)\n",
        "\n",
        "# Initial weights\n",
        "init_fig = plt.figure()\n",
        "agri_som_RK2.show_plot(init_fig, 1, 0)\n",
        "plt.show()\n",
        "\n",
        "agri_som_RK2.train(trunc_data.values,\n",
        "          num_epochs=200,\n",
        "          init_learning_rate=0.01\n",
        "          )"
      ],
      "metadata": {
        "id": "JwnytVF9b3vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, the classical SOM learning rule can be interpreted as an explicit Euler discretization of an underlying continuous-time dynamical system. By replacing this first-order integrator with a second-order Runge–Kutta scheme, the structure of the SOM algorithm remains unchanged, while the numerical integration of the governing ODE is improved.\n",
        "\n",
        "The proposed RK2-based update introduces a more accurate approximation of the continuous dynamics, leading to smoother and more stable weight trajectories during training. Although this modification entails a slightly higher computational cost per iteration, the observed behavior suggests improved numerical stability and more robust convergence properties.\n",
        "\n",
        "Therefore, from a numerical integration perspective, the use of a second-order Runge–Kutta method can be considered an improvement over the classical Euler-based SOM.\n",
        "\n",
        "A quantitative assessment of the error reduction and convergence rate associated with this higher-order integration scheme is addressed in the following question.\n"
      ],
      "metadata": {
        "id": "IbAstYPFci-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "## **Q8$^*$:** Estimate the absolute error after N epochs by using Q7."
      ],
      "metadata": {
        "id": "RPghGu3n__hK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building upon Q7, where the SOM update rule was reformulated using the second-order Runge-Kutta (RK2) integration scheme, we now estimate the absolute error after $N$ epochs.\n",
        "\n",
        "The RK2 method has a local truncation error of order $\\mathcal{O}(h^3)$, derived from the Taylor expansion where the neglected term is $\\frac{h^3}{6}y'''(\\xi)$. For the SOM with RK2 update:\n",
        "$$\n",
        "e_{\\text{local}}^{\\text{RK2}}(t) = \\frac{\\alpha(t)^3}{6} \\left|\\frac{d^3 w_k}{dt^3}\\right|\n",
        "$$\n",
        "\n",
        "The third derivative can be computed from the ODE $\\frac{dw_k}{dt} = h_{ck}(t)(x(t) - w_k(t))$:\n",
        "$$\n",
        "\\frac{d^3 w_k}{dt^3} = h_{ck}(t)^3(x(t) - w_k(t))\n",
        "$$\n",
        "\n",
        "The absolute error after N epochs using RK2 is:\n",
        "$$\n",
        "E_N^{\\text{RK2}} = \\sum_{t=1}^{T} e_{\\text{local}}^{\\text{RK2}}(t) = \\sum_{t=1}^{T} \\frac{\\alpha(t)^3}{6} \\left|h_{ck}(t)^3(x(t) - w_k(t))\\right|\n",
        "$$"
      ],
      "metadata": {
        "id": "nUmiR1uPliTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from our_som1_RK2 import SOM_RK2\n",
        "\n",
        "# data\n",
        "data = pd.read_csv(\"cash-crops-nepal.csv\")\n",
        "trunc_data = data[[\"Area\", \"Production\", \"Yield\"]]\n",
        "trunc_data = trunc_data / trunc_data.max()\n",
        "\n",
        "np.random.seed(42)\n",
        "som_rk2 = SOM_RK2(6, 6, 3)\n",
        "som_rk2.time_constant = 200 / np.log(som_rk2.init_radius)\n",
        "\n",
        "total_error_rk2 = 0.0\n",
        "num_epochs = 200\n",
        "init_learning_rate = 0.01\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    radius = som_rk2.decay_radius(epoch)\n",
        "    alpha = som_rk2.decay_learning_rate(init_learning_rate, epoch, num_epochs)\n",
        "\n",
        "    for idx, row in trunc_data.iterrows():\n",
        "        x_t = row.values\n",
        "        bmu, bmu_idx = som_rk2.find_bmu(x_t)\n",
        "\n",
        "        for i in range(6):\n",
        "            for j in range(6):\n",
        "                w_k = som_rk2.net[i, j, :]\n",
        "                w_dist = np.sum((np.array([i, j]) - bmu_idx) ** 2)\n",
        "\n",
        "                if w_dist <= radius ** 2:\n",
        "                    h_ck = np.exp(-w_dist / (2 * radius ** 2))\n",
        "\n",
        "                    # local error 3rd order rk\n",
        "                    local_error = (alpha**3 / 6) * np.sum(np.abs(h_ck**3 * (x_t - w_k)))\n",
        "                    total_error_rk2 += local_error\n",
        "\n",
        "                    # weight update\n",
        "                    k1 = h_ck * (x_t - w_k)\n",
        "                    w_star = w_k + alpha * k1\n",
        "                    k2 = h_ck * (x_t - w_star)\n",
        "                    som_rk2.net[i, j, :] = w_k + (alpha / 2) * (k1 + k2)\n",
        "\n",
        "print(f\"Estimated absolute error after N=200 epochs (RK2): {total_error_rk2:.6f}\")"
      ],
      "metadata": {
        "id": "oXYqmtn0lk8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "## **Q9:** How would you combine the answers to Q1-Q8, in order to suggest an improved version?"
      ],
      "metadata": {
        "id": "XmeAF3_N__-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "\n",
        "## References"
      ],
      "metadata": {
        "id": "VhM1lORQSZxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"ref1\">[1]</a>\n",
        "https://hal.science/hal-02182882/document\n",
        "\n",
        "<a id=\"ref2\">[2]</a>\n",
        "https://hal.science/hal-01796059/file/RIO_marie_mai_2018_HAL.pdf\n",
        "\n",
        "<a id=\"ref3\">[3]</a>\n",
        "https://search.r-project.org/CRAN/refmans/aweSOM/html/somQuality.html\n",
        "\n",
        "<a id=\"ref4\">[4]</a>\n",
        "https://www.columbia.edu/~ww2040/8100F16/RM51.pdf"
      ],
      "metadata": {
        "id": "17KloENySeCc"
      }
    }
  ]
}